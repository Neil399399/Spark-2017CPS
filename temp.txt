/***********************core-site.xml**********************************/
<property>
        <name>fs.default.name</name>
        <value>hdfs://8ae6d8a18f20:9000</value>
</property>
/************************yarn-site.xml*********************************/
<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
</property>
<property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred/ShuffleHandler</value>
</property>
<property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>8ae6d8a18f20:8025</value>
</property>
<property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>8ae6d8a18f20:8030</value>
</property>
<property>
        <name>yarn.resourcemanager.address</name>
        <value>8ae6d8a18f20:8050</value>
</property>

/*************************mapred-site.xml*********************************/
<property>
        <name>mapred.job.tracker</name>
        <value>8ae6d8a18f20:54311</value>
</property>
/***************************hdfs-site.xml*******************************/
<property>
        <name>dfs.replication</name>
        <value>1</value>
</property>
<property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/hadoop_data/hdfs/datanode</value>
</property>


/*****************************spark-env.sh******************************/
export SPARK_MASTER_IP=170.17.0.2
export SPARK_MASTER_CORES=1
export SPARK_WORKER_MEMORY=512m
export SPARK_EXECUTOR_INSTANCES=4

/root/anaconda2
export PATH=/root/anaconda2/bin:$PATH
export ANACONDA_PATH=/root/anaconda2

export PYSPARK_DRIVER_PYTHON=$ANACONDA_PATH/bin/ipython
export PYSPARK_PYTHON=$ANACONDA_PATH/bin/python

19fc0d0c2bf1
71cf544fa2ab

docker run -it --name spark-master -p 8088:8088 -p 50070:50070 -p 50010:50010 -p 4040:4040 -p 8042:8042 -p 8888:8888 spark/hadoop bash

pyspark --master spark://172.17.0.2:7077 --num-executors 1 --total-executor-cores=1 --executor-memory 512m
root@9624aafa10a4:~/pythonwork/ipynotebook# PYSPARK_DRIVER_PYTHON=~/anaconda3/bin/ipython PYTHON_DRIVER_PYTHON_OPTS="notebook" MASTER=spark://172.17.0.2:7077 pyspark --num-executors 1 --total-executor-cores=1 --executor-memory 2g